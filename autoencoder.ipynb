{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ubEmSpDZWxO5"
   },
   "source": [
    "Created by Dr. Omer Ishaq for Week 1 for Deep Learning for Perception Course at FAST Islamabad in 2019 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWznBIP5awES"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tn0SrZBib9dT"
   },
   "outputs": [],
   "source": [
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform = transforms.ToTensor())\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform = transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=mnist_trainset, batch_size=1000, shuffle=True)\n",
    "test_loader = DataLoader(dataset=mnist_testset, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uAH_62wNhZR0"
   },
   "outputs": [],
   "source": [
    "\n",
    "fc1 = nn.Linear(784, 520)\n",
    "sg1 = nn.ReLU() #nn.Sigmoid()\n",
    "fc2 = nn.Linear(520, 284)\n",
    "sg2 = nn.ReLU() #nn.Sigmoid()\n",
    "fc3 = nn.Linear(284, 100)\n",
    "sg3 = nn.ReLU() #nn.Sigmoid()\n",
    "fc4 = nn.Linear(100, 284)\n",
    "sg4 = nn.ReLU() #nn.Sigmoid()\n",
    "fc5 = nn.Linear(284, 520)\n",
    "sg5 = nn.ReLU() #nn.Sigmoid()\n",
    "fc6 = nn.Linear(520, 784)\n",
    "sg6 = nn.Sigmoid()\n",
    "\n",
    "model = torch.nn.Sequential(fc1, sg1, fc2, sg2, fc3, sg3, fc4, sg4, fc5, sg5, fc6, sg6)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "model = model.cuda()\n",
    "criterion = criterion.cuda()\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2bEh9Wi_0sY"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    epoch_loss = 0\n",
    "    for iteration, batch in enumerate(train_loader, 1):\n",
    "        input, target = Variable(batch[0]), Variable(batch[1])\n",
    "        input = input.cuda()\n",
    "        \n",
    "        input = input.view(1000,-1)\n",
    "        out = fc1.forward(input)\n",
    "        out = sg1.forward(out)\n",
    "        out = fc2.forward(out)\n",
    "        out = sg2.forward(out)\n",
    "        out = fc3.forward(out)\n",
    "        out = sg3.forward(out)\n",
    "        out = fc4.forward(out)\n",
    "        out = sg4.forward(out)\n",
    "        out = fc5.forward(out)\n",
    "        out = sg5.forward(out)\n",
    "        out = fc6.forward(out)\n",
    "        out = sg6.forward(out)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(out, input)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"===> Epoch {} Complete: Avg. Loss: {:.6f}\".format(epoch, epoch_loss / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "u7ltmwn0ApCt",
    "outputId": "8c8dfc8c-272b-424c-94cc-32eb396e5ad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 1 Complete: Avg. Loss: 0.006465\n",
      "===> Epoch 2 Complete: Avg. Loss: 0.006411\n",
      "===> Epoch 3 Complete: Avg. Loss: 0.006341\n",
      "===> Epoch 4 Complete: Avg. Loss: 0.006296\n",
      "===> Epoch 5 Complete: Avg. Loss: 0.006253\n",
      "===> Epoch 6 Complete: Avg. Loss: 0.006158\n",
      "===> Epoch 7 Complete: Avg. Loss: 0.006118\n",
      "===> Epoch 8 Complete: Avg. Loss: 0.006069\n",
      "===> Epoch 9 Complete: Avg. Loss: 0.005995\n",
      "===> Epoch 10 Complete: Avg. Loss: 0.005953\n",
      "===> Epoch 11 Complete: Avg. Loss: 0.005941\n",
      "===> Epoch 12 Complete: Avg. Loss: 0.005851\n",
      "===> Epoch 13 Complete: Avg. Loss: 0.005816\n",
      "===> Epoch 14 Complete: Avg. Loss: 0.005775\n",
      "===> Epoch 15 Complete: Avg. Loss: 0.005717\n",
      "===> Epoch 16 Complete: Avg. Loss: 0.005667\n",
      "===> Epoch 17 Complete: Avg. Loss: 0.005618\n",
      "===> Epoch 18 Complete: Avg. Loss: 0.005589\n",
      "===> Epoch 19 Complete: Avg. Loss: 0.005540\n",
      "===> Epoch 20 Complete: Avg. Loss: 0.005549\n",
      "===> Epoch 21 Complete: Avg. Loss: 0.005460\n",
      "===> Epoch 22 Complete: Avg. Loss: 0.005412\n",
      "===> Epoch 23 Complete: Avg. Loss: 0.005381\n",
      "===> Epoch 24 Complete: Avg. Loss: 0.005332\n",
      "===> Epoch 25 Complete: Avg. Loss: 0.005327\n",
      "===> Epoch 26 Complete: Avg. Loss: 0.005268\n",
      "===> Epoch 27 Complete: Avg. Loss: 0.005239\n",
      "===> Epoch 28 Complete: Avg. Loss: 0.005229\n",
      "===> Epoch 29 Complete: Avg. Loss: 0.005165\n",
      "===> Epoch 30 Complete: Avg. Loss: 0.005140\n",
      "===> Epoch 31 Complete: Avg. Loss: 0.005107\n",
      "===> Epoch 32 Complete: Avg. Loss: 0.005053\n",
      "===> Epoch 33 Complete: Avg. Loss: 0.005059\n",
      "===> Epoch 34 Complete: Avg. Loss: 0.004985\n",
      "===> Epoch 35 Complete: Avg. Loss: 0.004973\n",
      "===> Epoch 36 Complete: Avg. Loss: 0.004934\n",
      "===> Epoch 37 Complete: Avg. Loss: 0.004919\n",
      "===> Epoch 38 Complete: Avg. Loss: 0.004875\n",
      "===> Epoch 39 Complete: Avg. Loss: 0.004858\n",
      "===> Epoch 40 Complete: Avg. Loss: 0.004813\n",
      "===> Epoch 41 Complete: Avg. Loss: 0.004784\n",
      "===> Epoch 42 Complete: Avg. Loss: 0.004760\n",
      "===> Epoch 43 Complete: Avg. Loss: 0.004742\n",
      "===> Epoch 44 Complete: Avg. Loss: 0.004730\n",
      "===> Epoch 45 Complete: Avg. Loss: 0.004683\n",
      "===> Epoch 46 Complete: Avg. Loss: 0.004649\n",
      "===> Epoch 47 Complete: Avg. Loss: 0.004658\n",
      "===> Epoch 48 Complete: Avg. Loss: 0.004595\n",
      "===> Epoch 49 Complete: Avg. Loss: 0.004586\n",
      "===> Epoch 50 Complete: Avg. Loss: 0.004556\n",
      "===> Epoch 51 Complete: Avg. Loss: 0.004540\n",
      "===> Epoch 52 Complete: Avg. Loss: 0.004508\n",
      "===> Epoch 53 Complete: Avg. Loss: 0.004497\n",
      "===> Epoch 54 Complete: Avg. Loss: 0.004471\n",
      "===> Epoch 55 Complete: Avg. Loss: 0.004443\n",
      "===> Epoch 56 Complete: Avg. Loss: 0.004420\n",
      "===> Epoch 57 Complete: Avg. Loss: 0.004408\n",
      "===> Epoch 58 Complete: Avg. Loss: 0.004384\n",
      "===> Epoch 59 Complete: Avg. Loss: 0.004365\n",
      "===> Epoch 60 Complete: Avg. Loss: 0.004359\n",
      "===> Epoch 61 Complete: Avg. Loss: 0.004294\n",
      "===> Epoch 62 Complete: Avg. Loss: 0.004289\n",
      "===> Epoch 63 Complete: Avg. Loss: 0.004248\n",
      "===> Epoch 64 Complete: Avg. Loss: 0.004266\n",
      "===> Epoch 65 Complete: Avg. Loss: 0.004257\n",
      "===> Epoch 66 Complete: Avg. Loss: 0.004213\n",
      "===> Epoch 67 Complete: Avg. Loss: 0.004192\n",
      "===> Epoch 68 Complete: Avg. Loss: 0.004202\n",
      "===> Epoch 69 Complete: Avg. Loss: 0.004161\n",
      "===> Epoch 70 Complete: Avg. Loss: 0.004132\n",
      "===> Epoch 71 Complete: Avg. Loss: 0.004133\n",
      "===> Epoch 72 Complete: Avg. Loss: 0.004121\n",
      "===> Epoch 73 Complete: Avg. Loss: 0.004104\n",
      "===> Epoch 74 Complete: Avg. Loss: 0.004069\n",
      "===> Epoch 75 Complete: Avg. Loss: 0.004042\n",
      "===> Epoch 76 Complete: Avg. Loss: 0.004029\n",
      "===> Epoch 77 Complete: Avg. Loss: 0.003987\n",
      "===> Epoch 78 Complete: Avg. Loss: 0.004009\n",
      "===> Epoch 79 Complete: Avg. Loss: 0.003987\n",
      "===> Epoch 80 Complete: Avg. Loss: 0.003946\n",
      "===> Epoch 81 Complete: Avg. Loss: 0.003936\n",
      "===> Epoch 82 Complete: Avg. Loss: 0.003904\n",
      "===> Epoch 83 Complete: Avg. Loss: 0.003913\n",
      "===> Epoch 84 Complete: Avg. Loss: 0.003883\n",
      "===> Epoch 85 Complete: Avg. Loss: 0.003892\n",
      "===> Epoch 86 Complete: Avg. Loss: 0.003861\n",
      "===> Epoch 87 Complete: Avg. Loss: 0.003834\n",
      "===> Epoch 88 Complete: Avg. Loss: 0.003839\n",
      "===> Epoch 89 Complete: Avg. Loss: 0.003800\n",
      "===> Epoch 90 Complete: Avg. Loss: 0.003778\n",
      "===> Epoch 91 Complete: Avg. Loss: 0.003800\n",
      "===> Epoch 92 Complete: Avg. Loss: 0.003730\n",
      "===> Epoch 93 Complete: Avg. Loss: 0.003732\n",
      "===> Epoch 94 Complete: Avg. Loss: 0.003728\n",
      "===> Epoch 95 Complete: Avg. Loss: 0.003709\n",
      "===> Epoch 96 Complete: Avg. Loss: 0.003712\n",
      "===> Epoch 97 Complete: Avg. Loss: 0.003710\n",
      "===> Epoch 98 Complete: Avg. Loss: 0.003670\n",
      "===> Epoch 99 Complete: Avg. Loss: 0.003684\n",
      "===> Epoch 100 Complete: Avg. Loss: 0.003663\n",
      "===> Epoch 101 Complete: Avg. Loss: 0.003631\n",
      "===> Epoch 102 Complete: Avg. Loss: 0.003632\n",
      "===> Epoch 103 Complete: Avg. Loss: 0.003630\n",
      "===> Epoch 104 Complete: Avg. Loss: 0.003592\n",
      "===> Epoch 105 Complete: Avg. Loss: 0.003612\n",
      "===> Epoch 106 Complete: Avg. Loss: 0.003608\n",
      "===> Epoch 107 Complete: Avg. Loss: 0.003556\n",
      "===> Epoch 108 Complete: Avg. Loss: 0.003557\n",
      "===> Epoch 109 Complete: Avg. Loss: 0.003554\n",
      "===> Epoch 110 Complete: Avg. Loss: 0.003553\n",
      "===> Epoch 111 Complete: Avg. Loss: 0.003531\n",
      "===> Epoch 112 Complete: Avg. Loss: 0.003512\n",
      "===> Epoch 113 Complete: Avg. Loss: 0.003524\n",
      "===> Epoch 114 Complete: Avg. Loss: 0.003517\n",
      "===> Epoch 115 Complete: Avg. Loss: 0.003503\n",
      "===> Epoch 116 Complete: Avg. Loss: 0.003498\n",
      "===> Epoch 117 Complete: Avg. Loss: 0.003469\n",
      "===> Epoch 118 Complete: Avg. Loss: 0.003490\n",
      "===> Epoch 119 Complete: Avg. Loss: 0.003450\n",
      "===> Epoch 120 Complete: Avg. Loss: 0.003430\n",
      "===> Epoch 121 Complete: Avg. Loss: 0.003428\n",
      "===> Epoch 122 Complete: Avg. Loss: 0.003432\n",
      "===> Epoch 123 Complete: Avg. Loss: 0.003409\n",
      "===> Epoch 124 Complete: Avg. Loss: 0.003419\n",
      "===> Epoch 125 Complete: Avg. Loss: 0.003367\n",
      "===> Epoch 126 Complete: Avg. Loss: 0.003413\n",
      "===> Epoch 127 Complete: Avg. Loss: 0.003373\n",
      "===> Epoch 128 Complete: Avg. Loss: 0.003394\n",
      "===> Epoch 129 Complete: Avg. Loss: 0.003357\n",
      "===> Epoch 130 Complete: Avg. Loss: 0.003341\n",
      "===> Epoch 131 Complete: Avg. Loss: 0.003359\n",
      "===> Epoch 132 Complete: Avg. Loss: 0.003332\n",
      "===> Epoch 133 Complete: Avg. Loss: 0.003323\n",
      "===> Epoch 134 Complete: Avg. Loss: 0.003322\n",
      "===> Epoch 135 Complete: Avg. Loss: 0.003313\n",
      "===> Epoch 136 Complete: Avg. Loss: 0.003327\n",
      "===> Epoch 137 Complete: Avg. Loss: 0.003297\n",
      "===> Epoch 138 Complete: Avg. Loss: 0.003297\n",
      "===> Epoch 139 Complete: Avg. Loss: 0.003265\n",
      "===> Epoch 140 Complete: Avg. Loss: 0.003281\n",
      "===> Epoch 141 Complete: Avg. Loss: 0.003263\n",
      "===> Epoch 142 Complete: Avg. Loss: 0.003281\n",
      "===> Epoch 143 Complete: Avg. Loss: 0.003257\n",
      "===> Epoch 144 Complete: Avg. Loss: 0.003241\n",
      "===> Epoch 145 Complete: Avg. Loss: 0.003234\n",
      "===> Epoch 146 Complete: Avg. Loss: 0.003211\n",
      "===> Epoch 147 Complete: Avg. Loss: 0.003221\n",
      "===> Epoch 148 Complete: Avg. Loss: 0.003241\n",
      "===> Epoch 149 Complete: Avg. Loss: 0.003210\n",
      "===> Epoch 150 Complete: Avg. Loss: 0.003171\n",
      "===> Epoch 151 Complete: Avg. Loss: 0.003186\n",
      "===> Epoch 152 Complete: Avg. Loss: 0.003180\n",
      "===> Epoch 153 Complete: Avg. Loss: 0.003165\n",
      "===> Epoch 154 Complete: Avg. Loss: 0.003191\n",
      "===> Epoch 155 Complete: Avg. Loss: 0.003145\n",
      "===> Epoch 156 Complete: Avg. Loss: 0.003144\n",
      "===> Epoch 157 Complete: Avg. Loss: 0.003138\n",
      "===> Epoch 158 Complete: Avg. Loss: 0.003137\n",
      "===> Epoch 159 Complete: Avg. Loss: 0.003100\n",
      "===> Epoch 160 Complete: Avg. Loss: 0.003093\n",
      "===> Epoch 161 Complete: Avg. Loss: 0.003106\n",
      "===> Epoch 162 Complete: Avg. Loss: 0.003084\n",
      "===> Epoch 163 Complete: Avg. Loss: 0.003106\n",
      "===> Epoch 164 Complete: Avg. Loss: 0.003084\n",
      "===> Epoch 165 Complete: Avg. Loss: 0.003072\n",
      "===> Epoch 166 Complete: Avg. Loss: 0.003071\n",
      "===> Epoch 167 Complete: Avg. Loss: 0.003061\n",
      "===> Epoch 168 Complete: Avg. Loss: 0.003066\n",
      "===> Epoch 169 Complete: Avg. Loss: 0.003040\n",
      "===> Epoch 170 Complete: Avg. Loss: 0.003043\n",
      "===> Epoch 171 Complete: Avg. Loss: 0.003029\n",
      "===> Epoch 172 Complete: Avg. Loss: 0.003022\n",
      "===> Epoch 173 Complete: Avg. Loss: 0.003012\n",
      "===> Epoch 174 Complete: Avg. Loss: 0.003041\n",
      "===> Epoch 175 Complete: Avg. Loss: 0.003023\n",
      "===> Epoch 176 Complete: Avg. Loss: 0.003005\n",
      "===> Epoch 177 Complete: Avg. Loss: 0.003014\n",
      "===> Epoch 178 Complete: Avg. Loss: 0.003011\n",
      "===> Epoch 179 Complete: Avg. Loss: 0.002996\n",
      "===> Epoch 180 Complete: Avg. Loss: 0.002985\n",
      "===> Epoch 181 Complete: Avg. Loss: 0.002977\n",
      "===> Epoch 182 Complete: Avg. Loss: 0.002968\n",
      "===> Epoch 183 Complete: Avg. Loss: 0.002952\n",
      "===> Epoch 184 Complete: Avg. Loss: 0.002979\n",
      "===> Epoch 185 Complete: Avg. Loss: 0.002962\n",
      "===> Epoch 186 Complete: Avg. Loss: 0.002945\n",
      "===> Epoch 187 Complete: Avg. Loss: 0.002937\n",
      "===> Epoch 188 Complete: Avg. Loss: 0.002941\n",
      "===> Epoch 189 Complete: Avg. Loss: 0.002946\n",
      "===> Epoch 190 Complete: Avg. Loss: 0.002915\n",
      "===> Epoch 191 Complete: Avg. Loss: 0.002895\n",
      "===> Epoch 192 Complete: Avg. Loss: 0.002928\n",
      "===> Epoch 193 Complete: Avg. Loss: 0.002902\n",
      "===> Epoch 194 Complete: Avg. Loss: 0.002916\n",
      "===> Epoch 195 Complete: Avg. Loss: 0.002915\n",
      "===> Epoch 196 Complete: Avg. Loss: 0.002884\n",
      "===> Epoch 197 Complete: Avg. Loss: 0.002896\n",
      "===> Epoch 198 Complete: Avg. Loss: 0.002887\n",
      "===> Epoch 199 Complete: Avg. Loss: 0.002886\n",
      "===> Epoch 200 Complete: Avg. Loss: 0.002889\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-efaf8aff817c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-51-37fe41a7a96d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \"\"\"\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 400):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "zOFuyqbnQhxr",
    "outputId": "b59f9c9d-6b18-4898-9041-8680aee851ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 784])\n",
      "(28, 28)\n",
      "<class 'numpy.ndarray'>\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  23  24  25  28  33  34  39  41  43  44  46  48  49  51  53  59\n",
      "  60  61  62  63  66  74  75  79  87  88  89  93  95  97  99 101 102 110\n",
      " 114 121 123 131 133 136 137 141 142 144 146 152 156 160 164 165 166 167\n",
      " 172 175 177 179 180 188 192 193 194 201 204 206 209 210 212 215 219 220\n",
      " 222 223 224 227 228 234 236 237 238 241 242 243 245 246 247]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "for iteration, batch in enumerate(train_loader, 1):\n",
    "    input, target = Variable(batch[0]), Variable(batch[1])\n",
    "    input = input.cuda()\n",
    "\n",
    "    input = input.view(1000,-1)\n",
    "    out = fc1.forward(input)\n",
    "    out = sg1.forward(out)\n",
    "    out = fc2.forward(out)\n",
    "    out = sg2.forward(out)\n",
    "    out = fc3.forward(out)\n",
    "    out = sg3.forward(out)\n",
    "    out = fc4.forward(out)\n",
    "    out = sg4.forward(out)\n",
    "    out = fc5.forward(out)\n",
    "    out = sg5.forward(out)\n",
    "    out = fc6.forward(out)\n",
    "    out = sg6.forward(out)\n",
    "    \n",
    "    break\n",
    "    \n",
    "print(np.shape(out))\n",
    "img = out[5,:]\n",
    "img = img.view(28,28)\n",
    "img = img.cpu()\n",
    "img_array = img.detach().numpy()\n",
    "print(np.shape(img_array))\n",
    "print(type(img_array))\n",
    "img_array = img_array * 255\n",
    "img_array = np.uint8(img_array)\n",
    "print(np.unique(img_array))\n",
    "\n",
    "import cv2\n",
    "cv2.imwrite('abc.png', img_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZZPmO_CPShPM",
    "outputId": "8c2a1bc6-ad26-4d48-e971-d3424f7a3125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc.png  data  sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "from google.colab import files\n",
    "files.download('abc.png') "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "autoencoder.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
